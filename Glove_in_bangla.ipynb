{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Glove_in_bangla.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"pCrRU4DIHUZ5","colab_type":"code","colab":{}},"source":["# Generate auth tokens for Colab\n","from google.colab import auth\n","auth.authenticate_user()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"35VmsHDcL12Q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5710faa0-a1dc-4057-c41a-4798858263b1","executionInfo":{"status":"ok","timestamp":1561381720425,"user_tz":-360,"elapsed":4208,"user":{"displayName":"Ashik Ahmed Aman Rafat","photoUrl":"https://lh4.googleusercontent.com/-oBuZaknuekk/AAAAAAAAAAI/AAAAAAAAB1w/755uv_RShp8/s64/photo.jpg","userId":"14531663762273557902"}}},"source":["# Run this cell to mount your Google Drive.\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SjjqMR1SHowI","colab_type":"code","outputId":"a1276afb-0a79-446b-9841-fa44947db153","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1561381723455,"user_tz":-360,"elapsed":7221,"user":{"displayName":"Ashik Ahmed Aman Rafat","photoUrl":"https://lh4.googleusercontent.com/-oBuZaknuekk/AAAAAAAAAAI/AAAAAAAAB1w/755uv_RShp8/s64/photo.jpg","userId":"14531663762273557902"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","print(\"Files in Glove-in-bangla:\")\n","!ls /content/drive/My\\ Drive/Research/Vector/Glove-in-bangla"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Files in Glove-in-bangla:\n","corpus.txt\t       log\t\t saved_model\t  tf_glove.py\n","glove300d.txt\t       preprocessing.py  Siyamrupali.ttf\n","Glove_in_bangla.ipynb  __pycache__\t text3.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5X3zQYfEH-uJ","colab_type":"code","colab":{}},"source":["with open('/content/drive/My Drive/Research/Vector/Glove-in-bangla/corpus.txt') as f:\n","    singleLine = f.readline()\n","    listoflist = []\n","    words = []\n","    while singleLine:\n","        words = singleLine.split()\n","        listoflist.append(words)\n","        singleLine = f.readline()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wub--fdXnrjN","colab_type":"code","outputId":"52dfc3c9-c9b8-4610-9df7-012c83034a68","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1561381730202,"user_tz":-360,"elapsed":13925,"user":{"displayName":"Ashik Ahmed Aman Rafat","photoUrl":"https://lh4.googleusercontent.com/-oBuZaknuekk/AAAAAAAAAAI/AAAAAAAAB1w/755uv_RShp8/s64/photo.jpg","userId":"14531663762273557902"}}},"source":["%cd /content/drive/My\\ Drive/Research/Vector/Glove-in-bangla\n","!ls\n","import tf_glove"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Research/Vector/Glove-in-bangla\n","corpus.txt\t       log\t\t saved_model\t  tf_glove.py\n","glove300d.txt\t       preprocessing.py  Siyamrupali.ttf\n","Glove_in_bangla.ipynb  __pycache__\t text3.txt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4utdVxEUnrjQ","colab_type":"text"},"source":["# Instantiating the model\n","\n","To create a new GloVe model, simply call `tf_glove.GloVeModel()`:"]},{"cell_type":"code","metadata":{"id":"OUgq042PnrjR","colab_type":"code","colab":{}},"source":["model = tf_glove.GloVeModel(embedding_size=300, context_size=10, min_occurrences=25,\n","                            learning_rate=0.05, batch_size=512)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YFiWKTyQnrjT","colab_type":"text"},"source":["`GloVeModel()` has several parameters:\n","- **`embedding_size`**: the target dimensionality of the trained word representations. Typically between 50 and 300.\n","- **`context_size`**: how many tokens on either side of a given word to include in each context window. Can be either a tuple of two ints, indicating how many token on the left and right to include, or a single int, which will be interpreted to mean symmetric context.\n","- **`max_vocab_size`** *(Optional)*: the maximum size of the model's vocabulary. The model's vocabulary will be the most frequently occurring words in the corpus up to this amount. The default is 100,000.\n","- **`min_occurrences`** *(Optional)*: the minimum number of times a word must have appeared in the corpus to be included in the model's vocabulary. Default is 1.\n","- **`scaling_factor`** *(Optional)*: the alpha term in Eqn. 9 of Pennington et al.'s paper. Default is 3/4, which is the paper's recommendation\n","- **`cooccurrence_cap`** *(Optional)*: the x_max term in Eqn. 9 of Pennington et al.'s paper. Default is 100, which is the paper's recommendation\n","- **`batch_size`** *(Optional)*: the number of cooccurrences per minibatch of in training. Default is 512, which seems to work well on my machine. If training is very slow, consider playing with this.\n","- **`learning_rate`** *(Optional)*: the Adagrad learning rate used in training. Default is 0.05, which is the paper's recommendation"]},{"cell_type":"markdown","metadata":{"id":"ISkeXmUOnrjU","colab_type":"text"},"source":["# Reading the corpus\n","\n","tf_glove needs to be fit to a corpus in order to learn word representations. To do this, we'll use `GloVeModel.fit_to_corpus(corpus)`. This method expects an iterable of iterables of strings, where each string is a token, like this:\n","\n","`[[\"this\", \"is\", \"a\", \"comment\", \".\"], [\"this\", \"is\", \"another\", \"comment\", \".\"]]`\n","\n","That was a list of lists, but any iterable of iterables of strings should work.\n","\n","### Note on getting the dataset (if you want to follow along with these examples exactly)\n","\n","For these examples, I'm going to use the dataset of Reddit comments described here: https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment\n","\n","tf_glove is designed to work with any corpus, so there's no need to download this dataset. However, if you'd like to, that post has a link to a torrent for all of the comments as well as a link for just the comments from January 2015. Even just the January 2015 file is quite large (~5 GB).\n","\n","I downloaded it and used\n","\n","`$ head -n 1000000 RC_2015-01 > /path/to/RC_2015-01-1m_sample`\n","\n","to get the 1 million comment sample file referenced below. You could also use 100k if you want to save some time. 1 million comments takes ~15 minutes to fit on my machine.\n","\n","## The code:"]},{"cell_type":"code","metadata":{"id":"ekDnSVFinrjV","colab_type":"code","colab":{}},"source":["# import re\n","# import nltk\n","\n","# def extract_reddit_comments(path):\n","#     # A regex for extracting the comment body from one line of JSON (faster than parsing)\n","#     body_snatcher = re.compile(r\"\\{.*?(?<!\\\\)\\\"body(?<!\\\\)\\\":(?<!\\\\)\\\"(.*?)(?<!\\\\)\\\".*}\")\n","#     with open(path) as file_:\n","#         for line in file_:\n","#             match = body_snatcher.match(line)\n","#             if match:\n","#                 body = match.group(1)\n","#                 # Ignore deleted comments\n","#                 if not body == '[deleted]':\n","#                     # Return the comment as a string (not yet tokenized)\n","#                     yield body\n","                        \n","# def tokenize_comment(comment_str):\n","#     # Use the excellent NLTK to tokenize the comment body\n","#     #\n","#     # Note that we're lower-casing the comments here. tf_glove is case-sensitive,\n","#     # so if you want 'You' and 'you' to be considered the same word, be sure to lower-case everything.\n","#     return nltk.wordpunct_tokenize(comment_str.lower())\n","\n","# def reddit_comment_corpus(path):\n","#     # A generator that returns lists of tokens representing individual words in the comment\n","#     return (tokenize_comment(comment) for comment in extract_reddit_comments(path))\n","\n","# # Replace the path with the path to your corpus file\n","# corpus = reddit_comment_corpus(\"/media/grady/PrimeMover/Datasets/RC_2015-01-1m_sample\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nYxFcCkinrjX","colab_type":"text"},"source":["Now, to fit the model to the corpus:"]},{"cell_type":"code","metadata":{"id":"3E3AScVqnrjY","colab_type":"code","outputId":"715430cc-ef62-49d6-fe13-4a895db5dc07","colab":{"base_uri":"https://localhost:8080/","height":411},"executionInfo":{"status":"ok","timestamp":1561381849613,"user_tz":-360,"elapsed":133293,"user":{"displayName":"Ashik Ahmed Aman Rafat","photoUrl":"https://lh4.googleusercontent.com/-oBuZaknuekk/AAAAAAAAAAI/AAAAAAAAB1w/755uv_RShp8/s64/photo.jpg","userId":"14531663762273557902"}}},"source":["model.fit_to_corpus(listoflist)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0624 13:10:48.835576 140124413339520 deprecation_wrapper.py:119] From /content/drive/My Drive/Research/Vector/Glove-in-bangla/tf_glove.py:71: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0624 13:10:48.844144 140124413339520 deprecation_wrapper.py:119] From /content/drive/My Drive/Research/Vector/Glove-in-bangla/tf_glove.py:80: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","W0624 13:10:48.879179 140124413339520 deprecation.py:323] From /content/drive/My Drive/Research/Vector/Glove-in-bangla/tf_glove.py:99: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","W0624 13:10:48.886825 140124413339520 deprecation_wrapper.py:119] From /content/drive/My Drive/Research/Vector/Glove-in-bangla/tf_glove.py:104: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","W0624 13:10:48.887632 140124413339520 deprecation.py:323] From /content/drive/My Drive/Research/Vector/Glove-in-bangla/tf_glove.py:104: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","W0624 13:10:48.895706 140124413339520 deprecation_wrapper.py:119] From /content/drive/My Drive/Research/Vector/Glove-in-bangla/tf_glove.py:114: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n","\n","W0624 13:10:48.898762 140124413339520 deprecation_wrapper.py:119] From /content/drive/My Drive/Research/Vector/Glove-in-bangla/tf_glove.py:115: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n","\n","W0624 13:10:48.967634 140124413339520 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W0624 13:10:49.009383 140124413339520 deprecation_wrapper.py:119] From /content/drive/My Drive/Research/Vector/Glove-in-bangla/tf_glove.py:117: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n","\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YSMI_YGInrja","colab_type":"text"},"source":["# Training the model\n","\n","GloVeModel.fit_to_corpus() builds the vocabulary and cooccurrence matrix that will be used in training, but it doesn't actually train the word representations. It's time to kick off TensorFlow and train the model for real:"]},{"cell_type":"code","metadata":{"id":"UYSkazcqnrjb","colab_type":"code","outputId":"00b56b7e-0b82-4f12-a490-8803319f72fa","colab":{"base_uri":"https://localhost:8080/","height":1941},"executionInfo":{"status":"ok","timestamp":1561385214656,"user_tz":-360,"elapsed":3498323,"user":{"displayName":"Ashik Ahmed Aman Rafat","photoUrl":"https://lh4.googleusercontent.com/-oBuZaknuekk/AAAAAAAAAAI/AAAAAAAAB1w/755uv_RShp8/s64/photo.jpg","userId":"14531663762273557902"}}},"source":["model.train(num_epochs=50, checkpoint_dir=\"/content/drive/My Drive/Research/Vector/Glove-in-bangla/saved_model\", log_dir=\"log/example\", summary_batch_interval=1000)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["W0624 13:11:24.194761 140124413339520 deprecation_wrapper.py:119] From /content/drive/My Drive/Research/Vector/Glove-in-bangla/tf_glove.py:129: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","W0624 13:11:24.419865 140124413339520 deprecation_wrapper.py:119] From /content/drive/My Drive/Research/Vector/Glove-in-bangla/tf_glove.py:132: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n","W0624 13:11:24.452214 140124413339520 deprecation_wrapper.py:119] From /content/drive/My Drive/Research/Vector/Glove-in-bangla/tf_glove.py:133: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Writing TensorBoard summaries to log/example\n"],"name":"stdout"},{"output_type":"stream","text":["W0624 13:11:24.821895 140124413339520 deprecation_wrapper.py:119] From /content/drive/My Drive/Research/Vector/Glove-in-bangla/tf_glove.py:134: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch 0: Model is saved Elapsed: 00:01:06.76 Steps: 30535 Loss: 128.67724609375 \n","\n"," Epoch 1: Model is saved Elapsed: 00:02:11.54 Steps: 61070 Loss: 166.67669677734375 \n","\n"," Epoch 2: Model is saved Elapsed: 00:03:20.30 Steps: 91605 Loss: 82.757568359375 \n","\n"],"name":"stdout"},{"output_type":"stream","text":["W0624 13:15:58.054168 140124413339520 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n"],"name":"stderr"},{"output_type":"stream","text":[" Epoch 3: Model is saved Elapsed: 00:04:33.93 Steps: 122140 Loss: 82.68478393554688 \n","\n"," Epoch 4: Model is saved Elapsed: 00:05:49.15 Steps: 152675 Loss: 85.54630279541016 \n","\n"," Epoch 5: Model is saved Elapsed: 00:07:04.34 Steps: 183210 Loss: 40.807098388671875 \n","\n"," Epoch 6: Model is saved Elapsed: 00:08:20.08 Steps: 213745 Loss: 37.47620391845703 \n","\n"," Epoch 7: Model is saved Elapsed: 00:09:34.83 Steps: 244280 Loss: 44.592185974121094 \n","\n"," Epoch 8: Model is saved Elapsed: 00:10:48.76 Steps: 274815 Loss: 25.312870025634766 \n","\n"," Epoch 9: Model is saved Elapsed: 00:12:02.82 Steps: 305350 Loss: 30.420089721679688 \n","\n"," Epoch 10: Model is saved Elapsed: 00:13:16.14 Steps: 335885 Loss: 21.984752655029297 \n","\n"," Epoch 11: Model is saved Elapsed: 00:14:23.62 Steps: 366420 Loss: 29.131816864013672 \n","\n"," Epoch 12: Model is saved Elapsed: 00:15:36.91 Steps: 396955 Loss: 52.80020523071289 \n","\n"," Epoch 13: Model is saved Elapsed: 00:16:49.03 Steps: 427490 Loss: 19.578214645385742 \n","\n"," Epoch 14: Model is saved Elapsed: 00:17:54.59 Steps: 458025 Loss: 19.341899871826172 \n","\n"," Epoch 15: Model is saved Elapsed: 00:18:59.57 Steps: 488560 Loss: 17.957590103149414 \n","\n"," Epoch 16: Model is saved Elapsed: 00:20:04.49 Steps: 519095 Loss: 21.507583618164062 \n","\n"," Epoch 17: Model is saved Elapsed: 00:21:08.87 Steps: 549630 Loss: 25.54397964477539 \n","\n"," Epoch 18: Model is saved Elapsed: 00:22:13.24 Steps: 580165 Loss: 20.24881362915039 \n","\n"," Epoch 19: Model is saved Elapsed: 00:23:17.71 Steps: 610700 Loss: 17.62305450439453 \n","\n"," Epoch 20: Model is saved Elapsed: 00:24:22.51 Steps: 641235 Loss: 14.026969909667969 \n","\n"," Epoch 21: Model is saved Elapsed: 00:25:26.88 Steps: 671770 Loss: 15.29180908203125 \n","\n"," Epoch 22: Model is saved Elapsed: 00:26:30.92 Steps: 702305 Loss: 14.385416030883789 \n","\n"," Epoch 23: Model is saved Elapsed: 00:27:35.22 Steps: 732840 Loss: 14.85939884185791 \n","\n"," Epoch 24: Model is saved Elapsed: 00:28:40.18 Steps: 763375 Loss: 13.043352127075195 \n","\n"," Epoch 25: Model is saved Elapsed: 00:29:45.22 Steps: 793910 Loss: 9.163597106933594 \n","\n"," Epoch 26: Model is saved Elapsed: 00:30:49.62 Steps: 824445 Loss: 12.290780067443848 \n","\n"," Epoch 27: Model is saved Elapsed: 00:31:53.78 Steps: 854980 Loss: 13.911090850830078 \n","\n"," Epoch 28: Model is saved Elapsed: 00:32:56.83 Steps: 885515 Loss: 11.314619064331055 \n","\n"," Epoch 29: Model is saved Elapsed: 00:34:01.63 Steps: 916050 Loss: 21.323410034179688 \n","\n"," Epoch 30: Model is saved Elapsed: 00:35:05.87 Steps: 946585 Loss: 11.500752449035645 \n","\n"," Epoch 31: Model is saved Elapsed: 00:36:09.84 Steps: 977120 Loss: 26.529300689697266 \n","\n"," Epoch 32: Model is saved Elapsed: 00:37:13.79 Steps: 1007655 Loss: 10.70458984375 \n","\n"," Epoch 33: Model is saved Elapsed: 00:38:18.16 Steps: 1038190 Loss: 10.279746055603027 \n","\n"," Epoch 34: Model is saved Elapsed: 00:39:23.00 Steps: 1068725 Loss: 8.907814025878906 \n","\n"," Epoch 35: Model is saved Elapsed: 00:40:27.05 Steps: 1099260 Loss: 17.57561492919922 \n","\n"," Epoch 36: Model is saved Elapsed: 00:41:30.91 Steps: 1129795 Loss: 12.62310791015625 \n","\n"," Epoch 37: Model is saved Elapsed: 00:42:34.42 Steps: 1160330 Loss: 9.841117858886719 \n","\n"," Epoch 38: Model is saved Elapsed: 00:43:38.21 Steps: 1190865 Loss: 16.680038452148438 \n","\n"," Epoch 39: Model is saved Elapsed: 00:44:41.96 Steps: 1221400 Loss: 9.57694149017334 \n","\n"," Epoch 40: Model is saved Elapsed: 00:45:45.29 Steps: 1251935 Loss: 9.159975051879883 \n","\n"," Epoch 41: Model is saved Elapsed: 00:46:53.38 Steps: 1282470 Loss: 10.413561820983887 \n","\n"," Epoch 42: Model is saved Elapsed: 00:47:56.93 Steps: 1313005 Loss: 11.557699203491211 \n","\n"," Epoch 43: Model is saved Elapsed: 00:49:03.56 Steps: 1343540 Loss: 8.337111473083496 \n","\n"," Epoch 44: Model is saved Elapsed: 00:50:13.19 Steps: 1374075 Loss: 11.443716049194336 \n","\n"," Epoch 45: Model is saved Elapsed: 00:51:16.41 Steps: 1404610 Loss: 10.135736465454102 \n","\n"," Epoch 46: Model is saved Elapsed: 00:52:19.45 Steps: 1435145 Loss: 8.954858779907227 \n","\n"," Epoch 47: Model is saved Elapsed: 00:53:22.21 Steps: 1465680 Loss: 12.809840202331543 \n","\n"," Epoch 48: Model is saved Elapsed: 00:54:25.72 Steps: 1496215 Loss: 8.491456985473633 \n","\n"," Epoch 49: Model is saved Elapsed: 00:55:28.85 Steps: 1526750 Loss: 8.492975234985352 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TUzw15tYnrjd","colab_type":"text"},"source":["`GloVeModel.train()` has a few parameters:\n","- **`num_epochs`**: How many passes through the cooccurrence matrix the training should make. The paper recommends at least 50 for `embedding_size` < 300, and 100 otherwise.\n","- **`log_dir`** *(Optional)*: The path of the directory in which to log summaries for TensorBoard and t-SNE visualizations. Default is `None`, i.e. don't log anything.\n","- **`summary_batch_interval`** *(Optional)*: How many minibatches between logging events for TensorBoard. Default is 1000.\n","- **`tsne_epoch_interval`** *(Optional)*: How many epochs (full passes through cooccurrence matrix) between outputting a t-SNE visualization of the model's embeddings for the most frequent 1000 words in the vocabulary. Default is None, i.e. don't output t-SNE visualizations during training."]},{"cell_type":"markdown","metadata":{"id":"pVLu_vSonrje","colab_type":"text"},"source":["# Checking out the results\n","\n","Now that we've trained the model, let's look at the results.\n","\n","Use `GloVeModel.embedding_for()` to get the trained embedding for a single word:"]},{"cell_type":"code","metadata":{"id":"uTYNoeFFnrje","colab_type":"code","outputId":"784ece45-1ce2-4ea8-e96d-04e2d37d6596","colab":{"base_uri":"https://localhost:8080/","height":1309},"executionInfo":{"status":"ok","timestamp":1561385215167,"user_tz":-360,"elapsed":3498816,"user":{"displayName":"Ashik Ahmed Aman Rafat","photoUrl":"https://lh4.googleusercontent.com/-oBuZaknuekk/AAAAAAAAAAI/AAAAAAAAB1w/755uv_RShp8/s64/photo.jpg","userId":"14531663762273557902"}}},"source":["model.embedding_for(\"বাংলাদেশের\")"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 4.49468195e-01, -2.99610347e-01,  8.90811160e-02,  6.86227530e-02,\n","       -3.07363927e-01, -7.31231570e-02,  2.83948362e-01, -9.87636745e-02,\n","       -4.07758057e-02, -8.07990655e-02, -1.25963055e-02, -4.42586094e-02,\n","        4.51470554e-01, -3.67618442e-01, -2.33049601e-01, -2.48302251e-01,\n","       -2.53085226e-01,  2.99246192e-01,  3.13928545e-01,  2.41359413e-01,\n","       -7.82449469e-02, -4.76090550e-01, -1.18967630e-01,  7.86635652e-02,\n","       -1.26175612e-01,  3.86521101e-01,  6.25950247e-02,  2.94111490e-01,\n","       -1.48212135e-01, -2.84734219e-01, -2.19629630e-01,  1.12609729e-01,\n","       -3.82431746e-01,  1.22309178e-01, -7.11721256e-02, -2.61949241e-01,\n","       -2.93524474e-01,  4.14523423e-01,  1.44177318e-01,  7.76035786e-02,\n","        2.83498794e-01,  1.97772399e-01, -4.08547744e-02,  1.42682076e-01,\n","       -1.92996368e-01, -7.60802031e-02,  5.00685215e-01, -4.76303101e-01,\n","        8.83823931e-02,  4.09157723e-02, -1.98673308e-01, -3.77011180e-01,\n","        1.33172721e-01,  2.83148699e-02,  2.27745116e-01,  6.58346415e-01,\n","        1.20459266e-01,  4.88687575e-01,  8.49237144e-02,  1.54126808e-02,\n","       -9.88839492e-02, -2.09069818e-01, -2.26300359e-01,  7.81701207e-02,\n","        3.54260743e-01, -1.48456067e-01,  2.09406227e-01,  2.54517533e-02,\n","       -4.12511796e-01,  3.80114883e-01,  3.18051279e-02,  4.03864563e-01,\n","        3.71346235e-01,  2.38076568e-01,  4.13175374e-02,  9.86171216e-02,\n","       -4.77964431e-02,  1.14334881e-01,  3.34914386e-01,  6.59827888e-03,\n","       -1.34847701e-01, -1.08404003e-01, -7.58336484e-02, -2.65321195e-01,\n","        1.96117163e-01,  1.75652906e-01,  3.70322555e-01, -2.35729486e-01,\n","        1.02524593e-01, -1.95758000e-01, -2.62177199e-01, -1.61369950e-01,\n","        1.30550772e-01,  5.65800369e-01,  4.13171232e-01,  4.88661647e-01,\n","        3.50657344e-01,  1.13963783e-01,  2.56589949e-01,  1.20124504e-01,\n","       -3.17390561e-01,  5.98363169e-02, -1.28166184e-01,  2.07185760e-01,\n","        1.89678311e-01,  9.49813575e-02,  1.11002520e-01,  1.53972402e-01,\n","       -5.98933846e-02, -4.33244370e-02, -1.42709836e-02,  1.37329221e-01,\n","        2.23250359e-01,  9.28547606e-03,  3.74272823e-01,  1.43189371e-01,\n","       -2.45083362e-01,  1.62412152e-02, -2.33257804e-02,  1.57612607e-01,\n","       -8.25984254e-02,  1.93936095e-01,  1.10609494e-01,  3.48733544e-01,\n","       -5.56673184e-02, -2.91683793e-01,  1.31792322e-01, -3.26444358e-02,\n","        1.73990428e-01,  1.41810909e-01, -1.05453782e-01, -2.68812254e-02,\n","        4.05262351e-01, -6.95730746e-03, -1.29577622e-01,  2.04056874e-02,\n","       -6.91470504e-02, -3.07704896e-01, -2.25097001e-01,  7.26299882e-02,\n","       -9.26857293e-02, -1.72343731e-01, -4.35525775e-01, -3.87143970e-01,\n","       -1.72338217e-01, -2.81541586e-01, -5.84611773e-01,  3.07413846e-01,\n","        3.42850834e-01, -2.27741390e-01, -1.65668100e-01,  3.62814009e-01,\n","       -2.53389508e-01,  3.52870703e-01, -4.63488065e-02, -1.96780100e-01,\n","        1.10033259e-01, -1.46278292e-01, -2.23566014e-02,  1.36356950e-01,\n","       -3.35192680e-02, -4.27539758e-02, -4.06009972e-01,  5.25451452e-03,\n","        2.62485087e-01, -5.70422113e-02, -1.99878905e-02,  5.66006973e-02,\n","       -3.26003492e-01,  2.04006493e-01, -5.03759921e-01,  7.17790127e-01,\n","       -7.43455663e-02,  2.44685616e-02,  2.94042677e-02, -2.10848898e-01,\n","        1.11446589e-01,  3.73566687e-01,  6.60674095e-01,  1.24849729e-01,\n","       -2.19795361e-01, -1.23749681e-01,  3.41487452e-02, -4.07954454e-01,\n","       -2.46554315e-02,  3.12053353e-01,  3.21242571e-01, -2.33347908e-01,\n","       -3.10147822e-01, -1.21342212e-01,  1.73327088e-01,  2.15221390e-01,\n","       -1.89469784e-01, -2.81378273e-02,  9.61055905e-02,  3.52439582e-01,\n","       -6.84355572e-03, -1.72903538e-02, -9.98622924e-02, -1.14232302e-03,\n","        2.55515397e-01,  1.78938091e-01, -4.50441420e-01, -4.45528537e-01,\n","       -4.14325744e-01,  2.75968581e-01, -2.41683543e-01, -1.79152653e-01,\n","       -6.76944852e-04, -3.22956979e-01, -1.18227005e-02, -5.73886484e-02,\n","        2.73793370e-01,  2.65556633e-01,  3.43438268e-01,  8.09472948e-02,\n","        3.38739574e-01, -7.84491673e-02,  1.31775856e-01, -3.50931883e-01,\n","       -2.91559845e-03, -1.60634249e-01,  2.65826762e-01, -3.16449493e-01,\n","       -1.71215653e-01,  7.75267258e-02,  1.82404876e-01, -2.10405905e-02,\n","       -4.61576104e-01, -3.11883330e-01, -2.67117769e-01,  1.04515135e-01,\n","        1.37824193e-01, -2.96798408e-01, -6.82706237e-01,  5.72455883e-01,\n","        2.66770333e-01, -3.13254781e-02,  4.19739723e-01,  2.27830902e-01,\n","        5.58699667e-02,  2.45339349e-01, -1.99812680e-01,  2.88203597e-01,\n","        5.17428994e-01,  3.62139381e-02, -4.26847041e-01, -3.74583215e-01,\n","        2.34380662e-01, -4.36880708e-01, -4.27221239e-01,  2.85723716e-01,\n","        1.44179463e-01,  1.74518391e-01,  3.52986544e-01, -1.89432576e-01,\n","       -1.17326848e-01, -2.73653790e-02,  2.12356344e-01,  2.59698570e-01,\n","        1.37910187e-01, -4.68560964e-01, -2.82245606e-01, -4.24116313e-01,\n","        2.30908036e-01,  1.51492655e-02, -4.08349559e-02, -1.20978206e-01,\n","        3.71456623e-01, -7.14282990e-02, -5.07251441e-01,  2.30711758e-01,\n","       -6.61321759e-01,  1.47447139e-01, -2.12281793e-01, -2.28811532e-01,\n","       -1.08059254e-02,  3.25582027e-01,  5.17042875e-01,  2.20131710e-01,\n","        5.45331538e-01, -2.69494116e-01,  1.64077818e-01,  8.41495991e-02,\n","       -3.30309331e-01,  2.27554694e-01,  1.55072719e-01, -4.21164006e-01,\n","       -7.02492222e-02, -4.23137367e-01, -2.80393362e-01, -1.59991518e-01,\n","        1.70306899e-02, -1.73754603e-01, -1.86794996e-01, -2.90388793e-01,\n","        6.21212840e-01,  6.87544718e-02, -8.31457675e-02, -3.68397295e-01],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"dp3pq544nrjj","colab_type":"text"},"source":["You can also get the model's embeddings for every word in the vocabulary like this:"]},{"cell_type":"markdown","metadata":{"id":"-nCQrLERnrjo","colab_type":"text"},"source":["`GloVeModel.embeddings` will give you a NumPy matrix where each row is the model's embedding for a single word.\n","\n","To make use of this, you'll want to know what row corresponds to a particular word. You can do that with `GloVeModel.id_for_word`:"]},{"cell_type":"code","metadata":{"id":"KRuW2249nrjo","colab_type":"code","outputId":"2b3e0fe3-668c-4f03-b6c0-2467070e1258","colab":{"base_uri":"https://localhost:8080/","height":1309},"executionInfo":{"status":"ok","timestamp":1561385215171,"user_tz":-360,"elapsed":3498793,"user":{"displayName":"Ashik Ahmed Aman Rafat","photoUrl":"https://lh4.googleusercontent.com/-oBuZaknuekk/AAAAAAAAAAI/AAAAAAAAB1w/755uv_RShp8/s64/photo.jpg","userId":"14531663762273557902"}}},"source":["model.embeddings[model.id_for_word('বাংলাদেশের')]"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 4.49468195e-01, -2.99610347e-01,  8.90811160e-02,  6.86227530e-02,\n","       -3.07363927e-01, -7.31231570e-02,  2.83948362e-01, -9.87636745e-02,\n","       -4.07758057e-02, -8.07990655e-02, -1.25963055e-02, -4.42586094e-02,\n","        4.51470554e-01, -3.67618442e-01, -2.33049601e-01, -2.48302251e-01,\n","       -2.53085226e-01,  2.99246192e-01,  3.13928545e-01,  2.41359413e-01,\n","       -7.82449469e-02, -4.76090550e-01, -1.18967630e-01,  7.86635652e-02,\n","       -1.26175612e-01,  3.86521101e-01,  6.25950247e-02,  2.94111490e-01,\n","       -1.48212135e-01, -2.84734219e-01, -2.19629630e-01,  1.12609729e-01,\n","       -3.82431746e-01,  1.22309178e-01, -7.11721256e-02, -2.61949241e-01,\n","       -2.93524474e-01,  4.14523423e-01,  1.44177318e-01,  7.76035786e-02,\n","        2.83498794e-01,  1.97772399e-01, -4.08547744e-02,  1.42682076e-01,\n","       -1.92996368e-01, -7.60802031e-02,  5.00685215e-01, -4.76303101e-01,\n","        8.83823931e-02,  4.09157723e-02, -1.98673308e-01, -3.77011180e-01,\n","        1.33172721e-01,  2.83148699e-02,  2.27745116e-01,  6.58346415e-01,\n","        1.20459266e-01,  4.88687575e-01,  8.49237144e-02,  1.54126808e-02,\n","       -9.88839492e-02, -2.09069818e-01, -2.26300359e-01,  7.81701207e-02,\n","        3.54260743e-01, -1.48456067e-01,  2.09406227e-01,  2.54517533e-02,\n","       -4.12511796e-01,  3.80114883e-01,  3.18051279e-02,  4.03864563e-01,\n","        3.71346235e-01,  2.38076568e-01,  4.13175374e-02,  9.86171216e-02,\n","       -4.77964431e-02,  1.14334881e-01,  3.34914386e-01,  6.59827888e-03,\n","       -1.34847701e-01, -1.08404003e-01, -7.58336484e-02, -2.65321195e-01,\n","        1.96117163e-01,  1.75652906e-01,  3.70322555e-01, -2.35729486e-01,\n","        1.02524593e-01, -1.95758000e-01, -2.62177199e-01, -1.61369950e-01,\n","        1.30550772e-01,  5.65800369e-01,  4.13171232e-01,  4.88661647e-01,\n","        3.50657344e-01,  1.13963783e-01,  2.56589949e-01,  1.20124504e-01,\n","       -3.17390561e-01,  5.98363169e-02, -1.28166184e-01,  2.07185760e-01,\n","        1.89678311e-01,  9.49813575e-02,  1.11002520e-01,  1.53972402e-01,\n","       -5.98933846e-02, -4.33244370e-02, -1.42709836e-02,  1.37329221e-01,\n","        2.23250359e-01,  9.28547606e-03,  3.74272823e-01,  1.43189371e-01,\n","       -2.45083362e-01,  1.62412152e-02, -2.33257804e-02,  1.57612607e-01,\n","       -8.25984254e-02,  1.93936095e-01,  1.10609494e-01,  3.48733544e-01,\n","       -5.56673184e-02, -2.91683793e-01,  1.31792322e-01, -3.26444358e-02,\n","        1.73990428e-01,  1.41810909e-01, -1.05453782e-01, -2.68812254e-02,\n","        4.05262351e-01, -6.95730746e-03, -1.29577622e-01,  2.04056874e-02,\n","       -6.91470504e-02, -3.07704896e-01, -2.25097001e-01,  7.26299882e-02,\n","       -9.26857293e-02, -1.72343731e-01, -4.35525775e-01, -3.87143970e-01,\n","       -1.72338217e-01, -2.81541586e-01, -5.84611773e-01,  3.07413846e-01,\n","        3.42850834e-01, -2.27741390e-01, -1.65668100e-01,  3.62814009e-01,\n","       -2.53389508e-01,  3.52870703e-01, -4.63488065e-02, -1.96780100e-01,\n","        1.10033259e-01, -1.46278292e-01, -2.23566014e-02,  1.36356950e-01,\n","       -3.35192680e-02, -4.27539758e-02, -4.06009972e-01,  5.25451452e-03,\n","        2.62485087e-01, -5.70422113e-02, -1.99878905e-02,  5.66006973e-02,\n","       -3.26003492e-01,  2.04006493e-01, -5.03759921e-01,  7.17790127e-01,\n","       -7.43455663e-02,  2.44685616e-02,  2.94042677e-02, -2.10848898e-01,\n","        1.11446589e-01,  3.73566687e-01,  6.60674095e-01,  1.24849729e-01,\n","       -2.19795361e-01, -1.23749681e-01,  3.41487452e-02, -4.07954454e-01,\n","       -2.46554315e-02,  3.12053353e-01,  3.21242571e-01, -2.33347908e-01,\n","       -3.10147822e-01, -1.21342212e-01,  1.73327088e-01,  2.15221390e-01,\n","       -1.89469784e-01, -2.81378273e-02,  9.61055905e-02,  3.52439582e-01,\n","       -6.84355572e-03, -1.72903538e-02, -9.98622924e-02, -1.14232302e-03,\n","        2.55515397e-01,  1.78938091e-01, -4.50441420e-01, -4.45528537e-01,\n","       -4.14325744e-01,  2.75968581e-01, -2.41683543e-01, -1.79152653e-01,\n","       -6.76944852e-04, -3.22956979e-01, -1.18227005e-02, -5.73886484e-02,\n","        2.73793370e-01,  2.65556633e-01,  3.43438268e-01,  8.09472948e-02,\n","        3.38739574e-01, -7.84491673e-02,  1.31775856e-01, -3.50931883e-01,\n","       -2.91559845e-03, -1.60634249e-01,  2.65826762e-01, -3.16449493e-01,\n","       -1.71215653e-01,  7.75267258e-02,  1.82404876e-01, -2.10405905e-02,\n","       -4.61576104e-01, -3.11883330e-01, -2.67117769e-01,  1.04515135e-01,\n","        1.37824193e-01, -2.96798408e-01, -6.82706237e-01,  5.72455883e-01,\n","        2.66770333e-01, -3.13254781e-02,  4.19739723e-01,  2.27830902e-01,\n","        5.58699667e-02,  2.45339349e-01, -1.99812680e-01,  2.88203597e-01,\n","        5.17428994e-01,  3.62139381e-02, -4.26847041e-01, -3.74583215e-01,\n","        2.34380662e-01, -4.36880708e-01, -4.27221239e-01,  2.85723716e-01,\n","        1.44179463e-01,  1.74518391e-01,  3.52986544e-01, -1.89432576e-01,\n","       -1.17326848e-01, -2.73653790e-02,  2.12356344e-01,  2.59698570e-01,\n","        1.37910187e-01, -4.68560964e-01, -2.82245606e-01, -4.24116313e-01,\n","        2.30908036e-01,  1.51492655e-02, -4.08349559e-02, -1.20978206e-01,\n","        3.71456623e-01, -7.14282990e-02, -5.07251441e-01,  2.30711758e-01,\n","       -6.61321759e-01,  1.47447139e-01, -2.12281793e-01, -2.28811532e-01,\n","       -1.08059254e-02,  3.25582027e-01,  5.17042875e-01,  2.20131710e-01,\n","        5.45331538e-01, -2.69494116e-01,  1.64077818e-01,  8.41495991e-02,\n","       -3.30309331e-01,  2.27554694e-01,  1.55072719e-01, -4.21164006e-01,\n","       -7.02492222e-02, -4.23137367e-01, -2.80393362e-01, -1.59991518e-01,\n","        1.70306899e-02, -1.73754603e-01, -1.86794996e-01, -2.90388793e-01,\n","        6.21212840e-01,  6.87544718e-02, -8.31457675e-02, -3.68397295e-01],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"YbmJpnznb2On","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d345a323-f9e4-4219-b3af-b598cc37bd68","executionInfo":{"status":"ok","timestamp":1561385215172,"user_tz":-360,"elapsed":3498785,"user":{"displayName":"Ashik Ahmed Aman Rafat","photoUrl":"https://lh4.googleusercontent.com/-oBuZaknuekk/AAAAAAAAAAI/AAAAAAAAB1w/755uv_RShp8/s64/photo.jpg","userId":"14531663762273557902"}}},"source":["model.id_for_word('বাংলাদেশের')"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["88"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"EARCZ-Xbnrjj","colab_type":"code","outputId":"41c1aac2-0a61-4189-92a7-e68993f4c883","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1561386306477,"user_tz":-360,"elapsed":684,"user":{"displayName":"Ashik Ahmed Aman Rafat","photoUrl":"https://lh4.googleusercontent.com/-oBuZaknuekk/AAAAAAAAAAI/AAAAAAAAB1w/755uv_RShp8/s64/photo.jpg","userId":"14531663762273557902"}}},"source":["embeddings = model.embeddings\n","embeddings[88,:2]"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.4494682 , -0.29961035], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"ANxT4Jq5cEFI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9f1f787e-aa1c-4fcb-86de-5ad3e999e0ba","executionInfo":{"status":"ok","timestamp":1561386309157,"user_tz":-360,"elapsed":932,"user":{"displayName":"Ashik Ahmed Aman Rafat","photoUrl":"https://lh4.googleusercontent.com/-oBuZaknuekk/AAAAAAAAAAI/AAAAAAAAB1w/755uv_RShp8/s64/photo.jpg","userId":"14531663762273557902"}}},"source":["vocabSize = model.vocab_size\n","vocabSize"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["18689"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"gyhMwG0G3Lo6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5fc968fd-b345-46b4-e145-62b6160c861f","executionInfo":{"status":"ok","timestamp":1561386311564,"user_tz":-360,"elapsed":1096,"user":{"displayName":"Ashik Ahmed Aman Rafat","photoUrl":"https://lh4.googleusercontent.com/-oBuZaknuekk/AAAAAAAAAAI/AAAAAAAAB1w/755uv_RShp8/s64/photo.jpg","userId":"14531663762273557902"}}},"source":["vocabulary = model.words\n","vocabulary[88]"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'বাংলাদেশের'"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"OOxPLOsZ12xY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"aad0dbc8-3224-40a9-b215-ffe046be3258","executionInfo":{"status":"ok","timestamp":1561386314211,"user_tz":-360,"elapsed":1062,"user":{"displayName":"Ashik Ahmed Aman Rafat","photoUrl":"https://lh4.googleusercontent.com/-oBuZaknuekk/AAAAAAAAAAI/AAAAAAAAB1w/755uv_RShp8/s64/photo.jpg","userId":"14531663762273557902"}}},"source":["embeddings[88,:2]"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.4494682 , -0.29961035], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"DkxLeNKV2Vyh","colab_type":"code","colab":{}},"source":["    with open('/content/drive/My Drive/Research/Vector/Glove-in-bangla/glove300d.txt', 'w') as file_:\n","      for i in range(vocabSize):\n","        embed = embeddings[i, :]\n","        word = vocabulary[i]\n","        file_.write('%s %s\\n' % (word, ' '.join(map(str, embed))))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wfYiZfgVnrjr","colab_type":"text"},"source":["And if you want to see a 2D visualization of the learned vector space, you can use `GloVeModel.generate_tsne()`:"]},{"cell_type":"code","metadata":{"id":"ID7gBB4Hnrjs","colab_type":"code","outputId":"e000f3a7-3534-49d5-d566-c46bec14c536","colab":{"base_uri":"https://localhost:8080/","height":5506,"output_embedded_package_id":"1yA6BQEKT6H0kFX8VmkFFWb1eXnXRxu-e"},"executionInfo":{"status":"ok","timestamp":1561386428542,"user_tz":-360,"elapsed":86588,"user":{"displayName":"Ashik Ahmed Aman Rafat","photoUrl":"https://lh4.googleusercontent.com/-oBuZaknuekk/AAAAAAAAAAI/AAAAAAAAB1w/755uv_RShp8/s64/photo.jpg","userId":"14531663762273557902"}}},"source":["%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","import matplotlib\n","import matplotlib.font_manager\n","# !wget https://www.omicronlab.com/download/fonts/Siyamrupali.ttf\n","!cp Siyamrupali.ttf /usr/local/lib/python3.6/dist-packages/matplotlib/mpl-data/fonts/ttf/\n","!cp Siyamrupali.ttf /usr/share/fonts/truetype/\n","\n","matplotlib.font_manager._rebuild()\n","matplotlib.rc('font', family='Siyam Rupali')\n","model.generate_tsne()"],"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"hOt5dPwlnrjv","colab_type":"text"},"source":["You might want to open that image in a new tab.\n","\n","With no parameters, `GloVeModel.generate_tsne()` can be used interactively like in this notebook, but it also has parameters that will let you save the visualization to a file and adjust the size of the image and how many words appear:\n","\n","- **`path`** *(Optional)*: The path at which to save the generated PNG image. Default is None, which only really makes sense for interactive environments.\n","- **`size`** *(Optional)*: A tuple of (width, height) in *inches*. (Yeah, I know right? This is inherited from matplotlib.) Default is 100 x 100.  \n","- **`word_count`** *(Optional)*: How many words to plot in the visualization. Default is 1000, which works fairly well for a (100 x 100) visualization.\n"]}]}